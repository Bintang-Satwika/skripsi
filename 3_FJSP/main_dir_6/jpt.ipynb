{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "Conveyor: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Agent Position: 3\n",
      "Observation: [1, 2, 3, 4]\n",
      "Internal Flag: None\n",
      "---------\n",
      "After action 'wait':\n",
      "Conveyor: [10, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Observation: [10, 1, 2, 3]\n",
      "Internal Flag: waiting\n",
      "Reward: 0.5\n",
      "---------\n",
      "After action 'accept':\n",
      "Conveyor: [10, 1, 2, 0, 4, 5, 6, 7, 8, 9]\n",
      "Observation: [10, 1, 2, 0]\n",
      "Internal Flag: working\n",
      "Reward: 1.0\n",
      "---------\n",
      "After action 'decline':\n",
      "Conveyor: [10, 1, 2, 0, 4, 5, 6, 7, 8, 9]\n",
      "Observation: [10, 1, 2, 0]\n",
      "Internal Flag: None\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def move_conveyor(conveyor):\n",
    "    \"\"\"\n",
    "    Simulate the conveyor belt moving by one step.\n",
    "    We'll rotate the list so that the last element becomes first.\n",
    "    \"\"\"\n",
    "    conveyor = collections.deque(conveyor)\n",
    "    conveyor.rotate(1)  # moves all elements one position to the right\n",
    "    return list(conveyor)\n",
    "\n",
    "def extract_observation(conveyor, agent_position, window_size):\n",
    "    \"\"\"\n",
    "    Extracts the observation window for the agent.\n",
    "    The agent can observe the job at its own position and the preceding window_size-1 positions.\n",
    "    If there are fewer than window_size jobs available, pad with 0.\n",
    "    \"\"\"\n",
    "    observation = []\n",
    "    for i in range(window_size):\n",
    "        pos = (agent_position - i) % len(conveyor)\n",
    "        observation.append(conveyor[pos])\n",
    "    # Reverse so that the closest job is first in the list\n",
    "    observation = list(reversed(observation))\n",
    "    return observation\n",
    "\n",
    "def compute_wait_reward(state):\n",
    "    \"\"\"\n",
    "    Compute a dummy wait reward.\n",
    "    In the actual paper, this reward is scaled by the distance.\n",
    "    Here, we'll simply return a fixed reward for demonstration.\n",
    "    \"\"\"\n",
    "    return 0.5\n",
    "\n",
    "def compute_accept_reward(state):\n",
    "    \"\"\"\n",
    "    Compute a dummy accept reward.\n",
    "    \"\"\"\n",
    "    return 1.0\n",
    "\n",
    "def remove_job_at_position(conveyor, position):\n",
    "    \"\"\"\n",
    "    Simulate removing a job from the conveyor at a given position.\n",
    "    We'll set that position to 0 (empty) for simplicity.\n",
    "    \"\"\"\n",
    "    new_conveyor = conveyor.copy()\n",
    "    new_conveyor[position] = 0\n",
    "    return new_conveyor\n",
    "\n",
    "def environment_step(state, action):\n",
    "    \"\"\"\n",
    "    Simulates a single timestep in the environment.\n",
    "    `state` is a dictionary containing:\n",
    "      - 'conveyor': list representing jobs on the conveyor (0 means empty)\n",
    "      - 'agent_position': index where the agent is located\n",
    "      - 'window_size': how many positions the agent can observe (including its own)\n",
    "      - 'observation': the current observation window (list of job values)\n",
    "      - 'internal_flag': internal flag indicating the agent's internal decision (e.g., 'waiting')\n",
    "    `action` can be 'accept', 'wait', or 'decline'.\n",
    "    Returns the updated state and reward.\n",
    "    \"\"\"\n",
    "    next_state = state.copy()\n",
    "    reward = 0.0\n",
    "\n",
    "    if action == 'wait':\n",
    "        reward = compute_wait_reward(state)\n",
    "        # The agent remains idle; however, the conveyor moves.\n",
    "        next_state['conveyor'] = move_conveyor(state['conveyor'])\n",
    "        # Update the observation based on the new conveyor positions.\n",
    "        next_state['observation'] = extract_observation(\n",
    "            next_state['conveyor'], state['agent_position'], state['window_size'])\n",
    "        # Set internal flag to indicate waiting\n",
    "        next_state['internal_flag'] = 'waiting'\n",
    "    \n",
    "    elif action == 'accept':\n",
    "        reward = compute_accept_reward(state)\n",
    "        # Remove the job at the agent's position (simulate acceptance)\n",
    "        next_state['conveyor'] = remove_job_at_position(state['conveyor'], state['agent_position'])\n",
    "        # Update observation\n",
    "        next_state['observation'] = extract_observation(\n",
    "            next_state['conveyor'], state['agent_position'], state['window_size'])\n",
    "        # Change internal flag to indicate working (or processing)\n",
    "        next_state['internal_flag'] = 'working'\n",
    "    \n",
    "    elif action == 'decline':\n",
    "        # Decline action: remove the job at agent's position without waiting\n",
    "        next_state['conveyor'] = remove_job_at_position(state['conveyor'], state['agent_position'])\n",
    "        next_state['observation'] = extract_observation(\n",
    "            next_state['conveyor'], state['agent_position'], state['window_size'])\n",
    "        # Reset the internal flag\n",
    "        next_state['internal_flag'] = None\n",
    "        reward = 0.0\n",
    "    \n",
    "    return next_state, reward\n",
    "\n",
    "def main():\n",
    "    # Initialize a simple state:\n",
    "    # Conveyor is a list of 10 positions, with some jobs represented by non-zero integers.\n",
    "    initial_conveyor = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    agent_position = 3  # The agent is adjacent to position index 3\n",
    "    window_size = 4     # The agent observes 4 positions: its own plus 3 preceding positions\n",
    "    \n",
    "    state = {\n",
    "        'conveyor': initial_conveyor,\n",
    "        'agent_position': agent_position,\n",
    "        'window_size': window_size,\n",
    "        'observation': extract_observation(initial_conveyor, agent_position, window_size),\n",
    "        'internal_flag': None\n",
    "    }\n",
    "    \n",
    "    print(\"Initial State:\")\n",
    "    print(\"Conveyor:\", state['conveyor'])\n",
    "    print(\"Agent Position:\", state['agent_position'])\n",
    "    print(\"Observation:\", state['observation'])\n",
    "    print(\"Internal Flag:\", state['internal_flag'])\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate a wait action\n",
    "    action = 'wait'\n",
    "    next_state, reward = environment_step(state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state['conveyor'])\n",
    "    print(\"Observation:\", next_state['observation'])\n",
    "    print(\"Internal Flag:\", next_state['internal_flag'])\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate an accept action on the new state\n",
    "    action = 'accept'\n",
    "    next_state2, reward2 = environment_step(next_state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state2['conveyor'])\n",
    "    print(\"Observation:\", next_state2['observation'])\n",
    "    print(\"Internal Flag:\", next_state2['internal_flag'])\n",
    "    print(\"Reward:\", reward2)\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate a decline action on the new state\n",
    "    action = 'decline'\n",
    "    next_state3, reward3 = environment_step(next_state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state3['conveyor'])\n",
    "    print(\"Observation:\", next_state3['observation'])\n",
    "    print(\"Internal Flag:\", next_state3['internal_flag'])\n",
    "    print(\"Reward:\", reward3)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "Conveyor: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 0, 'global_status': [0, 0, 0], 'jobs_window': [1, 2, 3, 4]}\n",
      "Internal Flag: None\n",
      "---------\n",
      "After action 'wait':\n",
      "Conveyor: [10, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 0, 'global_status': [0, 0, 0], 'jobs_window': [10, 1, 2, 3]}\n",
      "Internal Flag: waiting\n",
      "Reward: 0.5\n",
      "---------\n",
      "After action 'accept':\n",
      "Conveyor: [10, 1, 2, 0, 4, 5, 6, 7, 8, 9]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 1, 'global_status': [0, 0, 0], 'jobs_window': [10, 1, 2, 0]}\n",
      "Internal Flag: working\n",
      "Reward: 1.0\n",
      "---------\n",
      "After action 'decline':\n",
      "Conveyor: [10, 1, 2, 0, 4, 5, 6, 7, 8, 9]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 0, 'global_status': [0, 0, 0], 'jobs_window': [10, 1, 2, 0]}\n",
      "Internal Flag: None\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def move_conveyor(conveyor):\n",
    "    \"\"\"\n",
    "    Simulate the conveyor belt moving by one step.\n",
    "    We'll rotate the list so that the last element becomes first.\n",
    "    \"\"\"\n",
    "    conveyor = collections.deque(conveyor)\n",
    "    conveyor.rotate(1)  # moves all elements one position to the right\n",
    "    return list(conveyor)\n",
    "\n",
    "def get_observation(conveyor, agent_position, window_size, \n",
    "                    agent_capabilities, current_op, global_status):\n",
    "    \"\"\"\n",
    "    Constructs the observation vector ω₍r,t₎ as defined in the paper.\n",
    "    \n",
    "    Parameters:\n",
    "      - agent_position (yᵣ): The fixed position (index) where the agent is located.\n",
    "      - agent_capabilities (Oᵣ): A list representing the set of operations the agent can perform.\n",
    "      - current_op (O₍r,t₎): The current operation in use (0 if idle).\n",
    "      - global_status (Zₜ): A list representing the statuses of all agents.\n",
    "      - conveyor: The list representing jobs on the conveyor. Each index corresponds to a section.\n",
    "      - window_size (w): The number of sections the agent can observe (its own plus w-1 preceding sections).\n",
    "      \n",
    "    Returns a dictionary representing the observation vector.\n",
    "    \"\"\"\n",
    "    # The agent can see its own section and the preceding window_size-1 sections.\n",
    "    jobs_window = []\n",
    "    for i in range(window_size):\n",
    "        pos = (agent_position - i) % len(conveyor)\n",
    "        jobs_window.append(conveyor[pos])\n",
    "    # For clarity, we reverse the window so that the closest (at agent_position) comes first.\n",
    "    jobs_window = list(reversed(jobs_window))\n",
    "    \n",
    "    observation = {\n",
    "        'agent_position': agent_position,    # yᵣ\n",
    "        'capabilities': agent_capabilities,    # Oᵣ\n",
    "        'current_operation': current_op,       # O₍r,t₎ (0 if idle)\n",
    "        'global_status': global_status,        # Zₜ (list of statuses for all agents)\n",
    "        'jobs_window': jobs_window             # [Ŝ₍yᵣ,t₎, Ŝ₍yᵣ₋1,t₎, …, Ŝ₍yᵣ₋w+1,t₎]\n",
    "    }\n",
    "    return observation\n",
    "\n",
    "def compute_wait_reward(state):\n",
    "    \"\"\"\n",
    "    Compute a dummy wait reward.\n",
    "    In the actual paper, this reward is scaled by the distance (number of sections).\n",
    "    Here, we return a fixed reward for demonstration.\n",
    "    \"\"\"\n",
    "    return 0.5\n",
    "\n",
    "def compute_accept_reward(state):\n",
    "    \"\"\"\n",
    "    Compute a dummy accept reward.\n",
    "    \"\"\"\n",
    "    return 1.0\n",
    "\n",
    "def remove_job_at_position(conveyor, position):\n",
    "    \"\"\"\n",
    "    Simulate removing a job from the conveyor at a given position.\n",
    "    We set that position to 0 (empty).\n",
    "    \"\"\"\n",
    "    new_conveyor = conveyor.copy()\n",
    "    new_conveyor[position] = 0\n",
    "    return new_conveyor\n",
    "\n",
    "def environment_step(state, action):\n",
    "    \"\"\"\n",
    "    Simulates one timestep in the environment.\n",
    "    \n",
    "    The state is a dictionary containing:\n",
    "      - 'conveyor': list of jobs (each position corresponds to Ŝ₍q,t₎)\n",
    "      - 'agent_position': fixed index yᵣ of the agent\n",
    "      - 'window_size': w (number of observable sections)\n",
    "      - 'capabilities': Oᵣ (agent's operation capabilities)\n",
    "      - 'current_operation': O₍r,t₎ (0 if idle)\n",
    "      - 'global_status': Zₜ (dummy list of statuses for all agents)\n",
    "      - 'internal_flag': internal marker for the agent (e.g. 'waiting')\n",
    "      - 'observation': current observation vector (dict)\n",
    "    \n",
    "    The action can be 'accept', 'wait', or 'decline'.\n",
    "    Returns the updated state and a reward.\n",
    "    \"\"\"\n",
    "    next_state = state.copy()\n",
    "    reward = 0.0\n",
    "    \n",
    "    if action == 'wait':\n",
    "        reward = compute_wait_reward(state)\n",
    "        # The conveyor moves.\n",
    "        next_state['conveyor'] = move_conveyor(state['conveyor'])\n",
    "        # The agent remains idle: current_operation remains 0.\n",
    "        # Update the observation using the new conveyor configuration.\n",
    "        next_state['observation'] = get_observation(\n",
    "            next_state['conveyor'],\n",
    "            state['agent_position'],\n",
    "            state['window_size'],\n",
    "            state['capabilities'],\n",
    "            state['current_operation'],\n",
    "            state['global_status']\n",
    "        )\n",
    "        # Set an internal flag to record that the agent is waiting.\n",
    "        next_state['internal_flag'] = 'waiting'\n",
    "    \n",
    "    elif action == 'accept':\n",
    "        reward = compute_accept_reward(state)\n",
    "        # Remove the job at the agent's position (simulate acceptance).\n",
    "        next_state['conveyor'] = remove_job_at_position(state['conveyor'], state['agent_position'])\n",
    "        # Update observation.\n",
    "        next_state['observation'] = get_observation(\n",
    "            next_state['conveyor'],\n",
    "            state['agent_position'],\n",
    "            state['window_size'],\n",
    "            state['capabilities'],\n",
    "            1,  # Assume 1 indicates the agent has started processing.\n",
    "            state['global_status']\n",
    "        )\n",
    "        # Update internal flag to indicate the agent is now working.\n",
    "        next_state['internal_flag'] = 'working'\n",
    "        # Update current_operation to a non-zero value (simulated here as 1).\n",
    "        next_state['current_operation'] = 1\n",
    "    \n",
    "    elif action == 'decline':\n",
    "        # Decline: remove the job from the agent's immediate view.\n",
    "        next_state['conveyor'] = remove_job_at_position(state['conveyor'], state['agent_position'])\n",
    "        # Update observation.\n",
    "        next_state['observation'] = get_observation(\n",
    "            next_state['conveyor'],\n",
    "            state['agent_position'],\n",
    "            state['window_size'],\n",
    "            state['capabilities'],\n",
    "            state['current_operation'],  # remains 0\n",
    "            state['global_status']\n",
    "        )\n",
    "        # Reset internal flag.\n",
    "        next_state['internal_flag'] = None\n",
    "        reward = 0.0\n",
    "    \n",
    "    return next_state, reward\n",
    "\n",
    "def main():\n",
    "    # Initialize a simple state.\n",
    "    # For example, we have a conveyor with 10 positions.\n",
    "    initial_conveyor = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    agent_position = 3        # Agent is located at index 3.\n",
    "    window_size = 4           # Agent observes 4 sections: from its own (index 3) and 3 preceding indices.\n",
    "    capabilities = [1, 2]     # Dummy operation capabilities.\n",
    "    current_op = 0            # Agent is idle initially (0 indicates no operation in use).\n",
    "    global_status = [0, 0, 0]  # Dummy global statuses for all agents (e.g., all idle).\n",
    "    \n",
    "    # Create the initial observation vector according to the paper.\n",
    "    observation = get_observation(initial_conveyor, agent_position, window_size,\n",
    "                                  capabilities, current_op, global_status)\n",
    "    \n",
    "    state = {\n",
    "        'conveyor': initial_conveyor,\n",
    "        'agent_position': agent_position,\n",
    "        'window_size': window_size,\n",
    "        'capabilities': capabilities,\n",
    "        'current_operation': current_op,\n",
    "        'global_status': global_status,\n",
    "        'internal_flag': None,\n",
    "        'observation': observation\n",
    "    }\n",
    "    \n",
    "    print(\"Initial State:\")\n",
    "    print(\"Conveyor:\", state['conveyor'])\n",
    "    print(\"Observation:\", state['observation'])\n",
    "    print(\"Internal Flag:\", state['internal_flag'])\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate a 'wait' action.\n",
    "    action = 'wait'\n",
    "    next_state, reward = environment_step(state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state['conveyor'])\n",
    "    print(\"Observation:\", next_state['observation'])\n",
    "    print(\"Internal Flag:\", next_state['internal_flag'])\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate an 'accept' action on the new state.\n",
    "    action = 'accept'\n",
    "    next_state2, reward2 = environment_step(next_state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state2['conveyor'])\n",
    "    print(\"Observation:\", next_state2['observation'])\n",
    "    print(\"Internal Flag:\", next_state2['internal_flag'])\n",
    "    print(\"Reward:\", reward2)\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate a 'decline' action on the new state.\n",
    "    action = 'decline'\n",
    "    next_state3, reward3 = environment_step(next_state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state3['conveyor'])\n",
    "    print(\"Observation:\", next_state3['observation'])\n",
    "    print(\"Internal Flag:\", next_state3['internal_flag'])\n",
    "    print(\"Reward:\", reward3)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "Conveyor: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 0, 'global_status': [0, 0, 0], 'jobs_window': [2, 3, 4]}\n",
      "Internal Flag: None\n",
      "---------\n",
      "After action 'wait_1':\n",
      "Conveyor: [10, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 0, 'global_status': [0, 0, 0], 'jobs_window': [1, 2, 3]}\n",
      "Internal Flag: waiting_for_distance_1\n",
      "Reward: 0.5\n",
      "---------\n",
      "After action 'accept':\n",
      "Conveyor: [10, 1, 2, 0, 4, 5, 6, 7, 8, 9]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 1, 'global_status': [0, 0, 0], 'jobs_window': [1, 2, 0]}\n",
      "Internal Flag: working\n",
      "Reward: 1.0\n",
      "---------\n",
      "After action 'decline':\n",
      "Conveyor: [1, 2, 3, 0, 5, 6, 7, 8, 9, 10]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 0, 'global_status': [0, 0, 0], 'jobs_window': [2, 3, 0]}\n",
      "Internal Flag: None\n",
      "Reward: 0.0\n",
      "---------\n",
      "After action 'continue':\n",
      "Conveyor: [10, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Observation: {'agent_position': 3, 'capabilities': [1, 2], 'current_operation': 0, 'global_status': [0, 0, 0], 'jobs_window': [1, 2, 3]}\n",
      "Internal Flag: None\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def move_conveyor(conveyor):\n",
    "    \"\"\"\n",
    "    Simulate the conveyor belt moving by one timestep.\n",
    "    We rotate the list so that the last element becomes the first.\n",
    "    \"\"\"\n",
    "    conveyor = collections.deque(conveyor)\n",
    "    conveyor.rotate(1)  # rotate right by one position\n",
    "    return list(conveyor)\n",
    "\n",
    "def get_observation(conveyor, agent_position, window_size, \n",
    "                    agent_capabilities, current_op, global_status):\n",
    "    \"\"\"\n",
    "    Constructs the observation vector ω₍r,t₎ as defined in the paper.\n",
    "    The observation includes:\n",
    "      - agent_position (yᵣ)\n",
    "      - agent_capabilities (Oᵣ)\n",
    "      - current_operation (O₍r,t₎, 0 if idle)\n",
    "      - global_status (Zₜ)\n",
    "      - jobs_window: [Ŝ₍yᵣ,t₎, Ŝ₍yᵣ₋1,t₎, …, Ŝ₍yᵣ₋w+1,t₎]\n",
    "    \"\"\"\n",
    "    jobs_window = []\n",
    "    for i in range(window_size):\n",
    "        pos = (agent_position - i) % len(conveyor)\n",
    "        jobs_window.append(conveyor[pos])\n",
    "    # Reverse so that the closest (at agent_position) comes first in the list.\n",
    "    jobs_window = list(reversed(jobs_window))\n",
    "    \n",
    "    observation = {\n",
    "        'agent_position': agent_position,    # yᵣ\n",
    "        'capabilities': agent_capabilities,    # Oᵣ\n",
    "        'current_operation': current_op,       # O₍r,t₎ (0 if idle)\n",
    "        'global_status': global_status,        # Zₜ (list for all agents)\n",
    "        'jobs_window': jobs_window             # [Ŝ₍yᵣ,t₎, Ŝ₍yᵣ₋1,t₎, Ŝ₍yᵣ₋2,t₎] for window_size=3\n",
    "    }\n",
    "    return observation\n",
    "\n",
    "def compute_wait_reward(state, distance):\n",
    "    \"\"\"\n",
    "    Compute a dummy wait reward.\n",
    "    In the paper, this reward is scaled by the distance x.\n",
    "    For demonstration, we return 0.5 divided by the distance.\n",
    "    \"\"\"\n",
    "    return 0.5 / distance\n",
    "\n",
    "def compute_accept_reward(state):\n",
    "    \"\"\"\n",
    "    Compute a dummy accept reward.\n",
    "    \"\"\"\n",
    "    return 1.0\n",
    "\n",
    "def remove_job_at_position(conveyor, position):\n",
    "    \"\"\"\n",
    "    Simulate removing a job from the conveyor at a given position.\n",
    "    For simplicity, we set that position to 0 (empty).\n",
    "    \"\"\"\n",
    "    new_conveyor = conveyor.copy()\n",
    "    new_conveyor[position] = 0\n",
    "    return new_conveyor\n",
    "\n",
    "def environment_step(state, action):\n",
    "    \"\"\"\n",
    "    Simulate one timestep in the environment.\n",
    "    The state is a dictionary containing:\n",
    "      - 'conveyor': list representing jobs on the conveyor (each section is Ŝ₍q,t₎)\n",
    "      - 'agent_position': fixed index yᵣ of the agent\n",
    "      - 'window_size': number of observable sections (w, here 3)\n",
    "      - 'capabilities': Oᵣ (dummy list)\n",
    "      - 'current_operation': O₍r,t₎ (0 if idle)\n",
    "      - 'global_status': Zₜ (dummy list for all agents)\n",
    "      - 'internal_flag': an internal marker (e.g., 'waiting')\n",
    "      - 'observation': current observation vector (dict)\n",
    "    \n",
    "    The action is a string. Allowed actions are:\n",
    "      'accept'         - Accept job at current position (A₀)\n",
    "      'wait_1' or 'wait_2'  - Wait for the job at distance 1 or 2 (A₁ or A₂)\n",
    "      'decline'        - Decline the current job (A_d)\n",
    "      'continue'       - Continue with no decision (A_c)\n",
    "    \n",
    "    Returns the updated state and a reward.\n",
    "    \"\"\"\n",
    "    next_state = state.copy()\n",
    "    reward = 0.0\n",
    "\n",
    "    if action.startswith('wait'):\n",
    "        # Extract the distance parameter from the action string, e.g., 'wait_1' means distance=1.\n",
    "        _, distance_str = action.split('_')\n",
    "        distance = int(distance_str)\n",
    "        reward = compute_wait_reward(state, distance)\n",
    "        \n",
    "        # In a wait action, the agent does not pick the job at its own section.\n",
    "        # Instead, it waits for the job at distance 'distance'.\n",
    "        # Meanwhile, the environment updates: the conveyor moves.\n",
    "        next_state['conveyor'] = move_conveyor(state['conveyor'])\n",
    "        # Update the observation using the new conveyor configuration.\n",
    "        next_state['observation'] = get_observation(\n",
    "            next_state['conveyor'],\n",
    "            state['agent_position'],\n",
    "            state['window_size'],\n",
    "            state['capabilities'],\n",
    "            state['current_operation'],  # remains 0 (idle)\n",
    "            state['global_status']\n",
    "        )\n",
    "        # Internally, record that the agent is waiting for the job at the given distance.\n",
    "        next_state['internal_flag'] = f'waiting_for_distance_{distance}'\n",
    "    \n",
    "    elif action == 'accept':\n",
    "        reward = compute_accept_reward(state)\n",
    "        # Accept action: the agent picks the job at its own section (position yᵣ).\n",
    "        next_state['conveyor'] = remove_job_at_position(state['conveyor'], state['agent_position'])\n",
    "        # Update the observation.\n",
    "        # Now, assume the agent starts working, so current_operation is set to 1.\n",
    "        next_state['observation'] = get_observation(\n",
    "            next_state['conveyor'],\n",
    "            state['agent_position'],\n",
    "            state['window_size'],\n",
    "            state['capabilities'],\n",
    "            1,  # non-zero indicates that the agent is processing the job\n",
    "            state['global_status']\n",
    "        )\n",
    "        next_state['internal_flag'] = 'working'\n",
    "        next_state['current_operation'] = 1\n",
    "    \n",
    "    elif action == 'decline':\n",
    "        # Decline action: the agent rejects the current job without waiting for any upcoming job.\n",
    "        next_state['conveyor'] = remove_job_at_position(state['conveyor'], state['agent_position'])\n",
    "        next_state['observation'] = get_observation(\n",
    "            next_state['conveyor'],\n",
    "            state['agent_position'],\n",
    "            state['window_size'],\n",
    "            state['capabilities'],\n",
    "            state['current_operation'],  # remains 0 (idle)\n",
    "            state['global_status']\n",
    "        )\n",
    "        next_state['internal_flag'] = None\n",
    "        reward = 0.0\n",
    "    \n",
    "    elif action == 'continue':\n",
    "        # Continue action: no decision is required.\n",
    "        # The agent simply observes the next state (the environment moves).\n",
    "        next_state['conveyor'] = move_conveyor(state['conveyor'])\n",
    "        next_state['observation'] = get_observation(\n",
    "            next_state['conveyor'],\n",
    "            state['agent_position'],\n",
    "            state['window_size'],\n",
    "            state['capabilities'],\n",
    "            state['current_operation'],  # remains unchanged\n",
    "            state['global_status']\n",
    "        )\n",
    "        # No change in the internal flag.\n",
    "        reward = 0.0\n",
    "    \n",
    "    return next_state, reward\n",
    "\n",
    "def main():\n",
    "    # Initial state setup:\n",
    "    # Let's assume a conveyor with 10 sections where jobs are represented by numbers.\n",
    "    initial_conveyor = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    agent_position = 3        # The agent is adjacent to section at index 3.\n",
    "    window_size = 3           # The agent observes 3 sections: its own and 2 preceding sections.\n",
    "    capabilities = [1, 2]     # Dummy operation capabilities.\n",
    "    current_op = 0            # Initially idle.\n",
    "    global_status = [0, 0, 0]  # Dummy statuses for all agents.\n",
    "    \n",
    "    # Build the initial observation vector.\n",
    "    observation = get_observation(initial_conveyor, agent_position, window_size,\n",
    "                                  capabilities, current_op, global_status)\n",
    "    \n",
    "    state = {\n",
    "        'conveyor': initial_conveyor,\n",
    "        'agent_position': agent_position,\n",
    "        'window_size': window_size,\n",
    "        'capabilities': capabilities,\n",
    "        'current_operation': current_op,\n",
    "        'global_status': global_status,\n",
    "        'internal_flag': None,\n",
    "        'observation': observation\n",
    "    }\n",
    "    \n",
    "    print(\"Initial State:\")\n",
    "    print(\"Conveyor:\", state['conveyor'])\n",
    "    print(\"Observation:\", state['observation'])\n",
    "    print(\"Internal Flag:\", state['internal_flag'])\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate a 'wait' action for distance 1.\n",
    "    action = 'wait_1'\n",
    "    next_state, reward = environment_step(state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state['conveyor'])\n",
    "    print(\"Observation:\", next_state['observation'])\n",
    "    print(\"Internal Flag:\", next_state['internal_flag'])\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate an 'accept' action on the resulting state.\n",
    "    action = 'accept'\n",
    "    next_state2, reward2 = environment_step(next_state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state2['conveyor'])\n",
    "    print(\"Observation:\", next_state2['observation'])\n",
    "    print(\"Internal Flag:\", next_state2['internal_flag'])\n",
    "    print(\"Reward:\", reward2)\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate a 'decline' action on the original state.\n",
    "    action = 'decline'\n",
    "    next_state3, reward3 = environment_step(state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state3['conveyor'])\n",
    "    print(\"Observation:\", next_state3['observation'])\n",
    "    print(\"Internal Flag:\", next_state3['internal_flag'])\n",
    "    print(\"Reward:\", reward3)\n",
    "    print(\"---------\")\n",
    "    \n",
    "    # Simulate a 'continue' action on the original state.\n",
    "    action = 'continue'\n",
    "    next_state4, reward4 = environment_step(state, action)\n",
    "    print(f\"After action '{action}':\")\n",
    "    print(\"Conveyor:\", next_state4['conveyor'])\n",
    "    print(\"Observation:\", next_state4['observation'])\n",
    "    print(\"Internal Flag:\", next_state4['internal_flag'])\n",
    "    print(\"Reward:\", reward4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
